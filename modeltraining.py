# -*- coding: utf-8 -*-
"""gpt2training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_tF20RSBRwHue_-R5JaRP6BW5va5iebj
"""

!pip install datasets transformers tokenizers torch

from datasets import load_dataset, Dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from transformers import TrainingArguments, Trainer
from tokenizers.normalizers import Normalizer
from tokenizers.normalizers import NFC
import unicodedata
import torch

# Load the therapy dataset
dataset = load_dataset("RishiKompelli/TherapyDataset")
dataset

# Select the first 40,000 rows for training
dataset['train'] = dataset['train'].select(range(30000))

# Split the 'train' split into train and test sets
train_test_split = dataset['train'].train_test_split(test_size=0.2, seed=42)  # Add seed for reproducibility

# Access the new train and test sets
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

# Update the original DatasetDict
dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})

dataset

def normalize_text(text):
    # Check if text is None
    if text is None:
        return None  # or return an empty string, depending on your needs
    # Check if text is a list
    if isinstance(text, list):
        return [normalize_text(item) for item in text]
    else:
        # Lowercase
        text = text.lower()
        # Unicode normalization (example: NFC)
        text = unicodedata.normalize('NFC', text)
        # Remove special characters if needed
        text = ''.join(e for e in text if e.isalnum() or e.isspace())
        return text

# Apply normalization to the dataset
dataset = dataset.map(lambda x: {
    'input': normalize_text(x['input']),
    'output': normalize_text(x['output'])
}, batched=True)

# Load a tokenizer
tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")

# Load GPT 2 model
model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")

# Add a padding token if it does not exist
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2") # Reload the model
model.resize_token_embeddings(len(tokenizer))

def tokenize_text(examples):
    # Filter out None values
    examples['input'] = [text if text is not None else '' for text in examples['input']]
    examples['output'] = [text if text is not None else '' for text in examples['output']]

    inputs = tokenizer(examples['input'], truncation=True, padding='max_length', max_length=512)
    labels = tokenizer(examples['output'], truncation=True, padding='max_length', max_length=512)
    inputs['labels'] = labels['input_ids']
    return inputs

dataset = dataset.map(tokenize_text, batched=True)

# Prepare the dataset for fine-tuning
dataset = dataset.with_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"],
    output_all_columns=True,
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,
    per_device_train_batch_size=4,  # Reduced batch size to 8
    per_device_eval_batch_size=16,  # Reduced eval batch size
    gradient_accumulation_steps=4, # Added gradient accumulation
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    save_steps=500,  # This will be ignored since save_strategy is 'epoch'
    learning_rate=5e-5,
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=lambda pred: {"loss": pred.loss},
)

torch.cuda.empty_cache()

# Fine-tune the model
trainer.train(resume_from_checkpoint=True)

trainer.evaluate()

model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

from google.colab import files
!zip -r fine_tuned_model.zip fine_tuned_model/
files.download('fine_tuned_model.zip')

from google.colab import drive
drive.mount('/content/drive')

!pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import hf_hub_download
from huggingface_hub import HfApi

api = HfApi()

# Replace "your-username/your-model-name" with your desired repository name
api.upload_folder(
    folder_path="./fine_tuned_model",
    repo_id="theabdulsaboor/gpt2-therapist-finetuned",
    repo_type="model",
)